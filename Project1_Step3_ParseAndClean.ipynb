{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnWppqPE2JXJ"
   },
   "source": [
    "## Project 1 Step 3 Parse, Clean, and Count Words\n",
    "In this notebook, we will mainly do the followings: \n",
    "- Read in all downloaded files of 10-K and 10-Q\n",
    "- Parse them and remove some unnecessary parts in the file using BeautifulSoup\n",
    "- Tokenize and lemmatize all words in the file\n",
    "- Remove stop words, non-English words, and words with digits\n",
    "- Use the word list and dictionaries (H4N, Fin-Neg) to calculate negative word term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByP_2cvT3TDF"
   },
   "source": [
    "### Import packages and list work paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vb4ZQHtrmc43",
    "outputId": "2e2cab67-8d16-448e-f792-7b6eb326c31b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pZHK6lZ7zWLq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwwJjamOsFS_",
    "outputId": "d69aa561-939d-4125-b113-3a2dd83d2426"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary sources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qAhd14qi1_oT"
   },
   "outputs": [],
   "source": [
    "data_path = \"/content/drive/MyDrive/Mini 5/Natural Language Processing/Project 1/data/\"\n",
    "data_path_10q = \"/content/drive/MyDrive/Mini 5/Natural Language Processing/Project 1/data/10Q/\"\n",
    "data_path_10k = \"/content/drive/MyDrive/Mini 5/Natural Language Processing/Project 1/data/10K/\"\n",
    "\n",
    "cik_lookup_filename = \"/content/drive/MyDrive/Mini 5/Natural Language Processing/Project 1/CIK_lookup_results_cleaned.csv\"\n",
    "sp500_constituents_path = \"/content/drive/MyDrive/Mini 5/Natural Language Processing/Project 1/sp500_constituents.csv\"\n",
    "sp500_id_path = \"/content/drive/MyDrive/Mini 5/Natural Language Processing/Project 1/sp500_w_addl_id.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMLyDRhC7-lR"
   },
   "source": [
    "### Define helper functions\n",
    "#### Parse the file and remove unnecessary elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sM1mFv2X74EY"
   },
   "outputs": [],
   "source": [
    "def parse_remove_tables(soup):\n",
    "  \"\"\"\n",
    "  Function purpose: remove elements with <TABLE> tag in the xml/html document, per author's instruction\n",
    "  \"\"\"\n",
    "  # remove table data\n",
    "  for td in soup.find_all(\"td\"):\n",
    "    td.decompose()\n",
    "  # remove table rows\n",
    "  for tr in soup.find_all(\"tr\"):\n",
    "    tr.decompose()\n",
    "  # remove tables\n",
    "  for t in soup.find_all(\"table\"):\n",
    "    t.decompose()\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1D9Ftsvp9mdh"
   },
   "outputs": [],
   "source": [
    "def parse_remove_link(soup):\n",
    "  \"\"\"\n",
    "  Function purpose: remove elements with <a ...> tag and hyperlinks\n",
    "  \"\"\"\n",
    "  for link in soup.find_all(\"a\"):\n",
    "    link.decompose()\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J6dLeS8l-_eb"
   },
   "outputs": [],
   "source": [
    "def parse_remove_xml(soup):\n",
    "  \"\"\"\n",
    "  Function purpose: remove elements with xml tag\n",
    "  \"\"\"\n",
    "  for x in soup.find_all(\"xml\"):\n",
    "    x.decompose()\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "88Nkpy3TpeOO"
   },
   "outputs": [],
   "source": [
    "def parse_remove_xbrl(soup):\n",
    "  \"\"\"\n",
    "  Function purpose: remove elements with xbrl tag\n",
    "  \"\"\"\n",
    "  for x in soup.find_all(\"xbrl\"):\n",
    "    x.decompose()\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X1FXTWGgpmsR"
   },
   "outputs": [],
   "source": [
    "def parse_translate_ascii(soup):\n",
    "  \"\"\"\n",
    "  Function purpose: translate \"encoded\" characters such as &NBSP to blank space or &AMP to & (back to ASCII form)\n",
    "  \"\"\"\n",
    "  # replace &amp with &\n",
    "  soup = re.sub(r'(?s)(&amp)[.|\\s]*', '&', soup)\n",
    "\n",
    "  # replace &nbsp with blank space\n",
    "  soup = re.sub(r'(?s)(&nbsp)[.|\\s]*', ' ', soup)\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or8CaFjh7cN3"
   },
   "source": [
    "#### Remove html/xml tags to convert file into usual string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ke1oJsnPr8ja"
   },
   "outputs": [],
   "source": [
    "def parse_remove_htmltags(soup):\n",
    "  \"\"\"\n",
    "  Function purpose: remove all html/xml tags because those are not meaningful language\n",
    "  \"\"\"\n",
    "  # remove html/xml tags\n",
    "  text = soup.get_text()\n",
    "  # convert all letters to lower case\n",
    "  text = text.lower()\n",
    "  # eliminate all punctuation and number characters\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvkZL_C27XvJ"
   },
   "source": [
    "#### Tokenize and remove non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6dH4qF6H7VhJ"
   },
   "outputs": [],
   "source": [
    "def tokenize_remove_nonenglish(text):\n",
    "  \"\"\"\n",
    "  Function purpose: tokenize our text and remove those non-English words from future consideration\n",
    "  \"\"\"\n",
    "  # get an English word set\n",
    "  words = set(nltk.corpus.words.words())\n",
    "  # check whether a word is an english word\n",
    "  text = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w in words or not w.isalpha())\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w24Nryq8DrN"
   },
   "source": [
    "#### Lemmatization of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pVXlSow78IrS"
   },
   "outputs": [],
   "source": [
    "def lemmatize_words(words): \n",
    "  \"\"\"\n",
    "  Function purpose: Lemmatize our word list\n",
    "  Source: https://towardsdatascience.com/nlp-in-the-stock-market-8760d062eb92\n",
    "  \"\"\"\n",
    "  lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
    "  return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YPsOLE2I-Xwb"
   },
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "  \"\"\"\n",
    "  Function purpose: Lemmatize the text\n",
    "  Source: https://towardsdatascience.com/nlp-in-the-stock-market-8760d062eb92\n",
    "  \"\"\"\n",
    "  word_pattern = re.compile('\\w+')\n",
    "  text_lemmatize = lemmatize_words(word_pattern.findall(text))\n",
    "  return text_lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPI30jvO-sGK"
   },
   "source": [
    "#### Remove stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ALVT4uzz_LJs"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text_lemmatize):\n",
    "  \"\"\"\n",
    "  Function purpose: \n",
    "  Source: https://towardsdatascience.com/nlp-in-the-stock-market-8760d062eb92\n",
    "  \"\"\"\n",
    "  lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
    "  text_lemmatize = [word for word in text_lemmatize if word not in lemma_english_stopwords]\n",
    "  return text_lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD6NJeYe_lO6"
   },
   "source": [
    "#### Only keep those words with at least 2 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ij39ibrL_qW1"
   },
   "outputs": [],
   "source": [
    "def remove_single_letter_words(text_lemmatize):\n",
    "  text_lemmatize = [word for word in text_lemmatize if len(word) > 1]\n",
    "  return text_lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmG0IRM3_4a_"
   },
   "source": [
    "#### Remove all words with digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "A5J2q9NQ_3wU"
   },
   "outputs": [],
   "source": [
    "def remove_words_with_digits(text_lemmatize):\n",
    "  text_lemmatize = [w for w in text_lemmatize if any(ch.isdigit() for ch in w) == False]\n",
    "  return text_lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OApoOwawAHCI"
   },
   "source": [
    "#### Streamline the parsing, cleaning and process of making word lists\n",
    "Here are two ways we shall consider. We will experiment two ways of parsing and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hvPFAVRXvbmY"
   },
   "outputs": [],
   "source": [
    "def clean_method1(file_path):\n",
    "  \"\"\"\n",
    "  Function purpose: clean the text only use beautifulsoup objects\n",
    "  \"\"\"\n",
    "  soup = BeautifulSoup(open(file_path, 'r', encoding='utf-8'), 'xml')\n",
    "  soup = parse_remove_tables(soup)\n",
    "  soup = parse_remove_link(soup)\n",
    "  # soup = parse_remove_xml(soup)\n",
    "  # soup = parse_remove_xbrl(soup)\n",
    "  text = parse_remove_htmltags(soup)\n",
    "  text = tokenize_remove_nonenglish(text)\n",
    "  text = lemmatization(text)\n",
    "  text = remove_stop_words(text)\n",
    "  text = remove_single_letter_words(text)\n",
    "  text = remove_words_with_digits(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nGYj9YRGAw0O"
   },
   "outputs": [],
   "source": [
    "def clean_method2(file_path):\n",
    "  soup = Path(file_path).read_text()\n",
    "  soup = re.sub(r'(?s)\\<SEC-HEADER\\>(.|\\s)*?\\<\\/SEC-HEADER\\>\\n', '', soup)\n",
    "  soup = re.sub(r'(?s)\\<DOCUMENT\\>\\n\\<TYPE>EX(.*?)\\<\\/DOCUMENT\\>\\n', '', soup, flags=re.DOTALL)\n",
    "  soup = re.sub(r'(?s)\\<DOCUMENT\\>\\n\\<TYPE>GRAPHIC(.*?)\\<\\/DOCUMENT\\>\\n', '', soup, flags=re.DOTALL)\n",
    "  soup = re.sub(r'(?s)\\<DOCUMENT\\>\\n\\<TYPE>ZIP(.*?)\\<\\/DOCUMENT\\>\\n', '', soup, flags=re.DOTALL)\n",
    "  soup = re.sub(r'(?s)\\<DOCUMENT\\>\\n\\<TYPE>EXCEL(.*?)\\<\\/DOCUMENT\\>\\n', '', soup, flags=re.DOTALL)\n",
    "  soup = parse_translate_ascii(soup)\n",
    "\n",
    "  soup2 = BeautifulSoup(soup, 'xml')\n",
    "  soup2 = parse_remove_tables(soup2)\n",
    "  soup2 = parse_remove_link(soup2)\n",
    "  text = parse_remove_htmltags(soup2)\n",
    "  text = tokenize_remove_nonenglish(text)\n",
    "  text = lemmatization(text)\n",
    "  text = remove_stop_words(text)\n",
    "  text = remove_single_letter_words(text)\n",
    "  text = remove_words_with_digits(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "k0F27pjiNm56"
   },
   "outputs": [],
   "source": [
    "def clean_a_file(file_path, which_method=True):\n",
    "  if which_method == True:\n",
    "    return clean_method1(file_path)\n",
    "  else:\n",
    "    return clean_method2(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EkdoN69n2AOb"
   },
   "outputs": [],
   "source": [
    "# example_path = os.path.join(data_path, \"example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "63SW_lKSQMtB"
   },
   "outputs": [],
   "source": [
    "# example_wl = clean_a_file(example_path)\n",
    "# print(len(example_wl))\n",
    "# example_wl2 = clean_a_file(example_path, False)\n",
    "# print(len(example_wl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fzkZsQxoQZIz"
   },
   "outputs": [],
   "source": [
    "# example10q_path = os.path.join(data_path, \"example10q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UbtDy98FSSgG"
   },
   "outputs": [],
   "source": [
    "# example10q_wl = clean_a_file(example10q_path)\n",
    "# print(len(example10q_wl))\n",
    "# example10q_wl2 = clean_a_file(example10q_path, False)\n",
    "# print(len(example10q_wl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ec60qT-lSX5Q"
   },
   "outputs": [],
   "source": [
    "# example_str = Path(example_path).read_text()\n",
    "# print(len(example_str))\n",
    "# example10q_str = Path(example10q_path).read_text()\n",
    "# print(len(example10q_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SAxk3jadwxl"
   },
   "source": [
    "#### Make word lists for files with different tickers and filing dates\n",
    "In this section, we will read in all 10-K and 10-Q files, parse and clean them, and save the word list generated. Since this step can take a long time to execute, we will save the result in JSON file for future use.\n",
    "Our goal is to generate such dictionary of dictionaries for easy and fast O(1) look up:\n",
    "\n",
    "{\"XXX\": {\"20190101\": [...],\n",
    "         \"20200101\": [...]\n",
    "        },\n",
    "\n",
    " \"YYY\": {\"20210101\": [...],\n",
    "         \"20220101\": [...]\n",
    "        },\n",
    "        \n",
    " \"ZZZ\": {\"20170101\": [...],\n",
    "         \"20160101\": [...]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-oTzkxcswv-q"
   },
   "outputs": [],
   "source": [
    "def pipeline_function_10k(some_tickers, i):\n",
    "  # Make dictionary of word list\n",
    "  dict_10k = {}\n",
    "  # Record files that cannot be cleaned using current algorithms\n",
    "  problem_files_10k = []\n",
    "  problem_cnt_10k = 0\n",
    "  success_cnt_10k = 0\n",
    "\n",
    "  for ticker in tqdm(some_tickers):\n",
    "    # list all 10-K files under a certain ticker and check whether it is from 2016-2021\n",
    "    file_list = os.listdir(os.path.join(data_path_10k, ticker))\n",
    "    # file_list = [x for x in file_list if int(x.split(\"-\")[1]) == year_range]\n",
    "    dict_for_a_ticker = {}\n",
    "    for fi in file_list:\n",
    "      # parse & clean the file, and make the word list\n",
    "      try: \n",
    "        with open(os.path.join(data_path_10k, ticker, fi), 'r') as fp:\n",
    "          file_header = [next(fp) for i in range(9)]\n",
    "          filing_date = str(file_header[7][-9:-1])\n",
    "        word_list = clean_a_file(os.path.join(data_path_10k, ticker, fi))\n",
    "        if len(word_list) < 500:\n",
    "          problem_cnt_10k += 1\n",
    "          problem_files_10k.append((ticker, fi))\n",
    "          continue\n",
    "        dict_for_a_ticker[filing_date] = word_list\n",
    "        success_cnt_10k += 1\n",
    "      except:\n",
    "        problem_cnt_10k += 1\n",
    "        # if the file cannot be processed for some reason, record its path\n",
    "        problem_files_10k.append((ticker, fi))\n",
    "    dict_10k[ticker] = dict_for_a_ticker\n",
    "  # save the result dictionary into a json file\n",
    "  with open(os.path.join(data_path, \"wordlist10k_{}.json\".format(str(i))), \"w\") as outfile_10k:\n",
    "    json.dump(dict_10k, outfile_10k)\n",
    "  # save the summary dictionary into a json file\n",
    "  dict_10k_summary = {\"problem_count\": problem_cnt_10k, \"success_count\": success_cnt_10k, \"problem_files\": problem_files_10k}\n",
    "  with open(os.path.join(data_path, \"wordlist10k_{}_summary.json\".format(str(i))), \"w\") as outfile_10k_summary:\n",
    "    json.dump(dict_10k_summary, outfile_10k_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all tickers under the 10-K directory and split tickers into several groups for excution due to RAM usage limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qJcthSMfx9Ta"
   },
   "outputs": [],
   "source": [
    "# List all tickers under the 10-K directory\n",
    "tickers_10k = os.listdir(data_path_10k)\n",
    "# Split into several groups to execute separately to reduce RAM usage\n",
    "begin_index = [i * (len(tickers_10k) // 5) for i in list(range(5))]\n",
    "end_index = begin_index[1:]\n",
    "end_index.append(len(tickers_10k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIqRlGOb0as8",
    "outputId": "21449185-e997-4a35-fb3d-1ed5e543b7ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 108, 216, 324, 432]\n",
      "[108, 216, 324, 432, 540]\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "print(begin_index)\n",
    "print(end_index)\n",
    "print(len(tickers_10k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate all tickers across groups splited above to calculcate each word list for files with different tickers and filing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2QTiT6f0xF0",
    "outputId": "31750851-cf21-4f6d-de10-b7e7d76d96b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [1:47:49<00:00, 59.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 0 - 108\n",
      "1 108 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [1:53:32<00:00, 63.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 108 - 216\n",
      "2 216 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [1:20:56<00:00, 44.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 216 - 324\n",
      "3 324 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [47:20<00:00, 26.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 324 - 432\n",
      "4 432 540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [15:34<00:00,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 432 - 540\n"
     ]
    }
   ],
   "source": [
    "for i, index_pair in enumerate(zip(begin_index, end_index)):\n",
    "  print(i, index_pair[0], index_pair[1])\n",
    "  some_tickers = tickers_10k[index_pair[0]:index_pair[1]]\n",
    "  pipeline_function_10k(some_tickers, i)\n",
    "  print(\"Finish {} - {}\".format(index_pair[0], index_pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Z57ZMO1sovwx"
   },
   "outputs": [],
   "source": [
    "# # Make dictionary of word list\n",
    "# dict_10k = {}\n",
    "# # List all tickers under the 10-K directory\n",
    "# tickers_10k = os.listdir(data_path_10k)[:3]\n",
    "# # Record files that cannot be cleaned using current algorithms\n",
    "# problem_files_10k = []\n",
    "# problem_cnt_10k = 0\n",
    "# success_cnt_10k = 0\n",
    "# # we only use 2016-2021 data per updated instructions\n",
    "# year_range = set(range(16, 22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ACIzkM0zTZbG"
   },
   "outputs": [],
   "source": [
    "# for ticker in tqdm(tickers_10k):\n",
    "#   # list all 10-K files under a certain ticker and check whether it is from 2016-2021\n",
    "#   file_list = os.listdir(os.path.join(data_path_10k, ticker))\n",
    "#   # file_list = [x for x in file_list if int(x.split(\"-\")[1]) == year_range]\n",
    "#   dict_for_a_ticker = {}\n",
    "#   for fi in file_list:\n",
    "#     # parse & clean the file, and make the word list\n",
    "#     try: \n",
    "#       with open(os.path.join(data_path_10k, ticker, fi), 'r') as fp:\n",
    "#         file_header = [next(fp) for i in range(9)]\n",
    "#         filing_date = str(file_header[7][-9:-1])\n",
    "#       word_list = clean_a_file(os.path.join(data_path_10k, ticker, fi))\n",
    "#       if len(word_list) < 500:\n",
    "#         problem_cnt_10k += 1\n",
    "#         problem_files_10k.append((ticker, fi))\n",
    "#         continue\n",
    "#       dict_for_a_ticker[filing_date] = word_list\n",
    "#       success_cnt_10k += 1\n",
    "#     except:\n",
    "#       problem_cnt_10k += 1\n",
    "#       # if the file cannot be processed for some reason, record its path\n",
    "#       problem_files_10k.append((ticker, fi))\n",
    "#   dict_10k[ticker] = dict_for_a_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jLXUFVXToPa2"
   },
   "outputs": [],
   "source": [
    "# # save the result dictionary into a json file\n",
    "# with open(os.path.join(data_path, \"wordlist10k_{}.json\".format()), \"w\") as outfile_10k:\n",
    "#   json.dump(dict_10k, outfile_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CGP4yvpJqLkF"
   },
   "outputs": [],
   "source": [
    "# for key, item in dict_10k.items():\n",
    "#   for _, i in item.items():\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "bJog-dliiHDQ"
   },
   "outputs": [],
   "source": [
    "# # Make dictionary of word list\n",
    "# dict_10q = {}\n",
    "# # List all tickers under the 10-K directory\n",
    "# tickers_10q = os.listdir(data_path_10q)\n",
    "# # Record files that cannot be cleaned using current algorithms\n",
    "# problem_files_10q = []\n",
    "# problem_cnt_10q = 0\n",
    "# success_cnt_10q = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-pYcfQ5gpx89"
   },
   "outputs": [],
   "source": [
    "# for ticker in tqdm(tickers_10q):\n",
    "#   # list all 10-Q files under a certain ticker and check whether it is from 2016-2021\n",
    "#   file_list = os.listdir(os.path.join(data_path_10q, ticker))\n",
    "#   # print(file_list)\n",
    "#   # file_list = [x for x in file_list if int(x.split(\"-\")[1]) in year_range]\n",
    "#   # print(file_list)\n",
    "#   dict_for_a_ticker = {}\n",
    "#   for fi in file_list:\n",
    "#     # parse & clean the file, and make the word list\n",
    "#     try: \n",
    "#       with open(os.path.join(data_path_10q, ticker, fi), 'r') as fp:\n",
    "#         file_header = [next(fp) for i in range(9)]\n",
    "#         filing_date = str(file_header[7][-9:-1])\n",
    "#       word_list = clean_a_file(os.path.join(data_path_10q, ticker, fi))\n",
    "#       if len(word_list) < 500:\n",
    "#         problem_cnt_10q += 1\n",
    "#         problem_files_10q.append((ticker, fi))\n",
    "#         continue\n",
    "#       dict_for_a_ticker[filing_date] = word_list\n",
    "#       success_cnt_10q += 1\n",
    "#     except:\n",
    "#       problem_cnt_10q += 1\n",
    "#       # if the file cannot be processed for some reason, record its path\n",
    "#       problem_files_10q.append((ticker, fi))\n",
    "#   dict_10q[ticker] = dict_for_a_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "0Kp2tZCRqEXo"
   },
   "outputs": [],
   "source": [
    "# # save the result dictionary into a json file\n",
    "# with open(os.path.join(data_path, \"wordlist10q.json\"), \"w\") as outfile_10q:\n",
    "#   json.dump(dict_10q, outfile_10q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nVljF1E5s_Ka"
   },
   "outputs": [],
   "source": [
    "# print(\"success number of 10-K: {}\".format(success_cnt_10k))\n",
    "# print(\"success number of 10-Q: {}\".format(success_cnt_10q))\n",
    "# print(\"failing number of 10-K: {}\".format(problem_cnt_10k))\n",
    "# print(\"failing number of 10-Q: {}\".format(problem_cnt_10q))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
